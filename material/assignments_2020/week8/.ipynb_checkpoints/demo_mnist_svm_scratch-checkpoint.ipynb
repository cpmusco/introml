{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo 6: SVM for MNIST Digit Recognition\n",
    "\n",
    "In this demo, you will learn to:\n",
    "* Load and display images\n",
    "* Formulate image classification problems\n",
    "* Explain the limitations of linear classifiers for image classification\n",
    "* Build a simple SVM image classifier \n",
    "* Save and load results using `pickle`.\n",
    "\n",
    "For data, we will use the classic [MNIST](https://en.wikipedia.org/wiki/MNIST_database) data set used to recognize hand-written digits.  The dataset was originally produced in the 1980s and is now widely-used in machine learning classes as a simple image classification problem.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Data\n",
    "\n",
    "First, we load the standard packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the MNIST dataset is so widely-used, it comes in as a build-in dataset in many packages. In this lab, we will download the data from the Tensorflow package -- See here for [installations instructions](https://www.tensorflow.org/install).  You can get the data from other sources as well. \n",
    "\n",
    "In the Tensorflow dataset, the training and test data are represented as arrays:\n",
    "\n",
    "     Xtr.shape = 60000 x 28 x 28\n",
    "     Xts.shape = 10000 x 28 x 28\n",
    "     \n",
    "The test data consists of `60000` images of size `28 x 28` pixels; the test data consists of `10000` images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xtr shape: (60000, 28, 28)\n",
      "Xts shape: (10000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "(Xtr,ytr),(Xts,yts) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "print('Xtr shape: %s' % str(Xtr.shape))\n",
    "print('Xts shape: %s' % str(Xts.shape))\n",
    "\n",
    "ntr = Xtr.shape[0]\n",
    "nts = Xts.shape[0]\n",
    "nrow = Xtr.shape[1]\n",
    "ncol = Xtr.shape[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each pixel value is from `[0,255]`.  For this lab, it will be convenient to recale the value to -1 to 1 and reshape it as a `ntr x npix` and `nts x npix`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "npix = nrow*ncol\n",
    "Xtr = (Xtr/255 - 0.5)\n",
    "Xtr = Xtr.reshape((ntr,npix))\n",
    "\n",
    "Xts = (Xts/255 - 0.5)\n",
    "Xts = Xts.reshape((nts,npix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plt_digit(x):\n",
    "    nrow = 28\n",
    "    ncol = 28\n",
    "    xsq = x.reshape((nrow,ncol))\n",
    "    plt.imshow(xsq,  cmap='Greys_r')\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "\n",
    "# Select random digits\n",
    "nplt = 4\n",
    "Iperm = np.random.permutation(ntr)\n",
    "\n",
    "# Plot the images using the subplot command\n",
    "for i in range(nplt):\n",
    "    ind = Iperm[i]\n",
    "    plt.subplot(1,nplt,i+1)\n",
    "    plt_digit(Xtr[ind,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying a Logistic Regression Classifier\n",
    "\n",
    "To classify the digits, we will first use a logistic classifier.  We select a small number of samples for training. Generally, you would use more training samples, but the optimizer is very slow.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntr1 = 5000\n",
    "Xtr1 = Xtr[Iperm[:ntr1],:]\n",
    "ytr1 = ytr[Iperm[:ntr1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we run the optimizer.  I have placed the `verbose=10` option so that you can see the progress.  It may not appear in the browser but in the command line where you launched jupyter notebook.  This can take several minutes and will likely say that it ran out of iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   13.5s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   13.5s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=500,\n",
       "                   multi_class='multinomial', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=10,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "logreg = linear_model.LogisticRegression(verbose=10, solver='lbfgs',\\\n",
    "                                         multi_class='multinomial',max_iter=500)\n",
    "logreg.fit(Xtr1,ytr1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Before continuing, since it takes so long to run the optimizer, let's save the results in a file.  You can use `pickle` module for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open( \"mnist_logreg.p\", \"wb\" ) as fp:\n",
    "    pickle.dump( [logreg, Xtr1, ytr1, Iperm],  fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can recover the objects via the `pickle.load` command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open( \"mnist_logreg.p\", \"rb\" ) as fp:\n",
    "    logreg, Xtr1, ytr1, Iperm = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can measure the accuracy on the test data.  Again, we will test on a small sub-sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuaracy = 0.904000\n"
     ]
    }
   ],
   "source": [
    "nts1 = 5000\n",
    "Iperm_ts = np.random.permutation(nts) \n",
    "Xts1 = Xts[Iperm_ts[:nts1],:]\n",
    "yts1 = yts[Iperm_ts[:nts1]]\n",
    "yhat = logreg.predict(Xts1)\n",
    "acc = np.mean(yhat == yts1)\n",
    "print('Accuaracy = {0:f}'.format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should get an accuracy of around 89%.  This may sounds OK, but it is actually very poor.  Had we increased the number of training samples, it would have improved to about 92%, but that is still very bad.  To illustrate, let's plot some of the errors.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel Logistic Regression Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytr8 = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ytr8' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-63918e526a23>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mlogreg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinear_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msolver\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'lbfgs'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlogreg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXtr1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mytr8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'ytr8' is not defined"
     ]
    }
   ],
   "source": [
    "logreg = linear_model.LogisticRegression(verbose=10, solver='lbfgs',max_iter=500)\n",
    "logreg.fit(Xtr1,ytr8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = logreg.predict(Xts1)\n",
    "acc = np.mean(yhat == yts8)\n",
    "print('Accuaracy = {0:f}'.format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nplt = 4\n",
    "Ierr = np.where(yts1 != yhat)[0]\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "for i in range(nplt):        \n",
    "    plt.subplot(1,nplt,i+1)        \n",
    "    ind = Ierr[i]    \n",
    "    plt_digit(Xts1[ind,:])        \n",
    "    title = 'true={0:d} est={1:d}'.format(yts1[ind].astype(int), yhat[ind].astype(int))\n",
    "    plt.title(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously, some of these digits are very easy to classify for a human.  We can get more fine-grained analysis of the digit errors by computing the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "C = confusion_matrix(yts1,yhat)\n",
    "\n",
    "# Normalize the confusion matrix\n",
    "Csum = np.sum(C,1)\n",
    "C = C / Csum[None,:]\n",
    "\n",
    "# Print the confusion matrix\n",
    "print(np.array_str(C, precision=3, suppress_small=True))\n",
    "plt.imshow(C, interpolation='none')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the Weights\n",
    "\n",
    "To see the problem with the logistic classifier, it is useful to plot the weights for each digit.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = logreg.coef_\n",
    "nlabel = W.shape[0]\n",
    "plt.figure(figsize=(10, 6))\n",
    "for i in range(nlabel):\n",
    "    plt.subplot(2,5,i+1)\n",
    "    plt_digit(W[i,:])\n",
    "    plt.title('{0:d}'.format(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What you see is that each weight is a very blurry version of the digit.  The blurriness is due to the fact that weight must correlate with all shifts, rotations and other variations of the digits.  As a result, the weights begin to correlate with other incorrect digits leading to poor performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run an SVM classifier\n",
    "\n",
    "We now try an SVM classifier.  The parameters are given by \n",
    "\n",
    "https://martin-thoma.com/svm-with-sklearn/\n",
    "\n",
    "This website has a nice summary of the main equations for SVM as well.  That site trained on 40000 samples and tested on 20000.  But, to make this run faster, we will train on 10000 and test on 10000.  If you increase to 40000 training samples, you can get past 99% accuracy.\n",
    "\n",
    "First, we import the SVM package and construct the SVC with the parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First let us try the linear kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "# Create a classifier: a support vector classifier\n",
    "# svc = svm.SVC(probability=False,  kernel=\"rbf\", C=2.8, gamma=.0073,verbose=10)\n",
    "svc = svm.SVC(probability=False,  kernel=\"linear\", C=2.8, gamma=.0073,verbose=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we get the training and test data.  The features are re-scaled to be between -1 and 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We fit the training data.  Again, this will take several minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntr1 = 5000\n",
    "nts1 = 5000\n",
    "Xtr1 = Xtr[Iperm[:ntr1],:]\n",
    "ytr1 = ytr[Iperm[:ntr1]]\n",
    "Xts1 = Xts[Iperm_ts[:nts1],:]\n",
    "yts1 = yts[Iperm_ts[:nts1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc.fit(Xtr,ytr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the results in case you want them without re-running the above the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open( \"mnist_svc.p\", \"wb\" ) as fp:\n",
    "    pickle.dump( [svc, Xtr1,ytr1], fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can reload the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can skip this step if you run the classifier directly\n",
    "import pickle\n",
    "with open( \"mnist_svc.p\", \"rb\" ) as fp:\n",
    "    svc, Xtr1,ytr1 = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Measure the accuracy on the test data.  The prediction can take several minutes too -- SVMs are *very* slow!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_ts = svc.predict(Xts1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since even the prediction (sometimes called inference) is slow with SVMs, we will save the results in `pickle` file. Instead of running the prediction again, you can recapture the data with the following comamnd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"mnist_svc_test.p\", \"wb\") as fp:\n",
    "    pickle.dump([Xts1,yts1,yhat_ts], fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"mnist_svc_test.p\", \"rb\") as fp:\n",
    "    Xts1,yts1,yhat_ts = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = np.mean(yhat_ts == yts1)\n",
    "print('Accuaracy = {0:f}'.format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should get an accuracy of around 93%.  Again, had you trained on all 50,000 samples, it would have been much better -- close to 98.5%.  But, even this result is much better than logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at the Support Vectors\n",
    "\n",
    "Let's take a look at the support vectors.  We see there about over 2500 support vectors.  So, about quarter the training samples were used as SVs.  This is partly why the prediction was so slow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = svc.support_vectors_\n",
    "S.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us plot some support vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nplt = 4\n",
    "nsv = S.shape[0]\n",
    "Iperms = np.random.permutation(nsv)\n",
    "plt.figure(figsize=(10, 4))\n",
    "for i in range(nplt):        \n",
    "    plt.subplot(1,nplt,i+1)        \n",
    "    ind = Iperms[i]\n",
    "    plt_digit(S[ind,:])   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the support vectors look like digits we want to recognize. They are like the 'match filters'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let us see the performanc of SVM using the RBF kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytr8 = 1*(ytr1==8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yts8 = 1*(yts1==8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a classifier: a support vector classifier\n",
    "svcrbf = svm.SVC(probability=False,  kernel=\"rbf\", C=2.8, gamma=.0073,verbose=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can skip this step if loading from previous result \n",
    "svcrbf.fit(Xtr1,ytr8)\n",
    "import pickle\n",
    "with open( \"mnist_svcrbf.p\", \"wb\" ) as fp:\n",
    "    pickle.dump( [svcrbf, Xtr1, ytr1], fp)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can skip if you run the previous cell\n",
    "import pickle\n",
    "with open( \"mnist_svcrbf.p\", \"rb\" ) as fp:\n",
    "    svcrbf, Xtr1, ytr1 = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# skip if load from saved result\n",
    "yhat_ts = svcrbf.predict(Xts1)\n",
    "\n",
    "with open(\"mnist_svcrbf_test.p\", \"wb\") as fp:\n",
    "    pickle.dump([yhat_ts,yts1,Xts1], fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# skip if you run the previous cell\n",
    "with open(\"mnist_svcrbf_test.p\", \"rb\") as fp:\n",
    "    yhat_ts,yts1,Xts1 = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = np.mean(yhat_ts == yts8)\n",
    "print('Accuaracy = {0:f}'.format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the RBF kernel provides more accurate results. \n",
    "Now let us plot some errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ierr = np.where((yhat_ts != yts1))[0]\n",
    "nplt = 4\n",
    "plt.figure(figsize=(10, 4))\n",
    "for i in range(nplt):        \n",
    "    plt.subplot(1,nplt,i+1)        \n",
    "    ind = Ierr[i]    \n",
    "    plt_digit(Xts1[ind,:])        \n",
    "    title = 'true={0:d} est={1:d}'.format(yts1[ind].astype(int), yhat_ts[ind].astype(int))\n",
    "    plt.title(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that again a human would not have made these errors, but the digits in error are much less clear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at the Support Vectors\n",
    "\n",
    "Let's take a look at the support vectors.  We see there about 5000 support vectors.  So, about half the training samples were used as SVs, more than that for the linear kernel.  This is partly why the prediction was so slow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = svcrbf.support_vectors_\n",
    "S.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We can plot some of the vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nplt = 4\n",
    "nsv = S.shape[0]\n",
    "Iperms = np.random.permutation(nsv)\n",
    "plt.figure(figsize=(10, 4))\n",
    "for i in range(nplt):        \n",
    "    plt.subplot(1,nplt,i+1)        \n",
    "    ind = Iperms[i]\n",
    "    plt_digit(S[ind,:])        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In this demo, we specified the parameters for the SVC. In the lab, you will be asked to find the optimal parameters through cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_opt_adapt(grad_func, beta0, nit=1000, lr_init=1e-3):\n",
    "    \"\"\"\n",
    "    Gradient descent optimization with adaptive step size\n",
    "    \n",
    "    feval:  A function that returns f, fgrad, the objective\n",
    "            function and its gradient\n",
    "    beta0:  Initial estimate\n",
    "    nit:    Number of iterations\n",
    "    lr:     Initial learning rate\n",
    "    \"\"\"\n",
    "    \n",
    "    # Set initial point\n",
    "    beta = beta0\n",
    "    lr = lr_init\n",
    "    \n",
    "    # Create history dictionary for tracking progress per iteration.\n",
    "    # This isn't necessary if you just want the final answer, but it \n",
    "    # is useful for debugging\n",
    "    hist = {'lr': [], 'beta': [], 'L': []}\n",
    "\n",
    "    L,Lgrad = grad_func(beta0)\n",
    "    for it in range(nit):\n",
    "\n",
    "        # Take a gradient step\n",
    "        beta1 = beta - lr*Lgrad\n",
    "\n",
    "        # Evaluate the test point by computing the objective function, L1,\n",
    "        # at the test point and the predicted decrease, df_est\n",
    "        L1, Lgrad1 = grad_func(beta1)\n",
    "        df_est = Lgrad.T@(beta1-beta)\n",
    "        \n",
    "        # Check if test point passes the Armijo condition\n",
    "        alpha = 0.5\n",
    "        if (L1-L < alpha*df_est) and (L1 < L):\n",
    "            # If descent is sufficient, accept the point and increase the learning rate\n",
    "            lr = lr*2\n",
    "            L = L1\n",
    "            Lgrad = Lgrad1\n",
    "            beta = beta1\n",
    "        else:\n",
    "            # Otherwise, decrease the learning rate\n",
    "            lr = lr/2            \n",
    "            \n",
    "        # Save history\n",
    "        hist['L'].append(L)\n",
    "        hist['lr'].append(lr)\n",
    "        hist['beta'].append(beta0)\n",
    "\n",
    "    # Convert to numpy arrays\n",
    "    for elem in ('L', 'lr', 'beta'):\n",
    "        hist[elem] = np.array(hist[elem])\n",
    "    return beta, L, hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Leval_reg_kernel(alpha,K,y,lamb):\n",
    "    \"\"\"\n",
    "    Compute the loss and gradient given beta,X,y\n",
    "    \"\"\"\n",
    "    z = K@alpha\n",
    "    h = 1/(1+np.exp(-z))\n",
    "    L = np.sum((1-y)*z - np.log(h)) + lamb*np.sum(z*alpha)\n",
    "\n",
    "    # Gradient\n",
    "    Lgrad = K@(h-y) + 2*lamb*z\n",
    "    return L, Lgrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Take a random initial point\n",
    "p = K.shape[1]\n",
    "alpha0 = np.zeros(p)\n",
    "\n",
    "# Perturb the point\n",
    "step = 1e-3\n",
    "alpha1 = alpha0 + step*np.random.randn(p)\n",
    "\n",
    "# Measure the function and gradient at w0 and w1\n",
    "lamb = 10\n",
    "L0, Lgrad0 = Leval_reg_kernel(alpha0,K,ytr8,lamb)\n",
    "L1, Lgrad1 = Leval_reg_kernel(alpha1,K,ytr8,lamb)\n",
    "\n",
    "# Predict the amount the function should have changed based on the gradient\n",
    "dL_est = Lgrad0.T@(alpha1-alpha0)\n",
    "\n",
    "# Print the two values to see if they are close\n",
    "print(\"Actual L1-L0    = %12.4e\" % (L1-L0))\n",
    "print(\"Predicted L1-L0 = %12.4e\" % dL_est)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import pairwise_kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = .01\n",
    "K = pairwise_kernels(Xtr1, Xtr1, metric='rbf', gamma=g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.flip(np.sort(K[1,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Leval_reg_kernel_param = lambda alpha: Leval_reg_kernel(alpha,K,ytr8,.2)\n",
    "alpha0 = np.zeros(p)\n",
    "nit = 10000\n",
    "alpha, L, hist = grad_opt_adapt(Leval_reg_kernel_param, alpha0, nit=nit, lr_init=1e-5)\n",
    "\n",
    "t = np.arange(nit)\n",
    "plt.subplot(2,1,1)\n",
    "plt.loglog(t, hist['L'])\n",
    "plt.grid()\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "plt.subplot(2,1,2)\n",
    "plt.loglog(t, hist['lr'])\n",
    "plt.grid()\n",
    "plt.ylabel('Learning rate')\n",
    "plt.xlabel('Iteration')\n",
    "plt.tight_layout()\n",
    "\n",
    "yhat = (K@alpha > 0)\n",
    "acc = np.mean(yhat == ytr8)\n",
    "print(\"Test accuracy = %f\" % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xts1a = np.concatenate((np.ones((Xts1.shape[0],1)),Xtr1),axis=1)\n",
    "Kts = pairwise_kernels(Xts1, Xtr1, metric='rbf', gamma=g)\n",
    "yhat = (Kts@alpha > 0)\n",
    "acc = np.mean(yhat == yts8)\n",
    "print(\"Test accuracy = %f\" % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yts8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(yts8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yts8.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "487/5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
