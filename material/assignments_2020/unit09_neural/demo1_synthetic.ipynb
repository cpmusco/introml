{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A First Neural Network Example\n",
    "\n",
    "In this demo, you will learn:\n",
    "* How to construct and train a simple neural network with one hidden layer using the `keras` package\n",
    "* How to get the weights and intermediate layer outputs of a `keras` network after training\n",
    "* How to visualize the weights \n",
    "\n",
    "To illustrate the concepts, we consider a simple 2D classification problem on completely synthetic data. Using synthetic data will allow us to visualize the network more easily.  We will then look at real data in later demos  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Tensorflow and Keras\n",
    "\n",
    "Before starting this demo, you will need to install [Tensorflow](https://www.tensorflow.org/install/).  If you are using [Google colaboratory](https://colab.research.google.com), Tensorflw is already installed.\n",
    "\n",
    "Tensorflow is a powerful and widely-used platform for deep learning.  However, Tensorflow is relatively low level and may be a somewhat difficult to use as a starting point.  In this class, we will use the `keras` package which acts as a high-level wrapper on top of tensorflow that allows you to quickly build and fit models.  In 2018, `keras` was included as part of tensorflow and you do not need to separately install it. Building most neural networks in `keras` is generally much simpler than in raw Tensorflow and is perfect for getting started.  Later, if you want more flexibility, you can learn how to build models in Tensorflow directly.\n",
    "\n",
    "First we check that tensorflow is installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following line you can ignore. It was needed to properly use the current version of Tensorflow on my Macbook \n",
    "# due to issues with OpenMP. Leaving here in case it's useful for others.\n",
    "#import os\n",
    "# os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also load out other standard packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Synthetic Data \n",
    "\n",
    "To illustrate the neural network we generate data with some rule that will create an interesting classification region. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nsamp = 400   # number of samples to generate\n",
    "nx = 2        # number of dimensions of each sample\n",
    "\n",
    "# The features are generated uniformly on the square [0,1] x [0,1]\n",
    "X = np.random.uniform(0,1,(nsamp,nx))\n",
    "\n",
    "# The class of each sample is determined by a Gaussian.  The particular function is not important.\n",
    "rsq = (X[:,0]-0.5)**2 + (X[:,1]-0.5)**2\n",
    "z = 10*(np.exp(-8*rsq)-0.5)\n",
    "py = 1/(1+np.exp(-z))\n",
    "u = np.random.uniform(0,1,nsamp)\n",
    "y = (u < py).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a scatter plot of the data.  You can see that it is not linearly seperable.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "I0 = np.where(y==0)[0]\n",
    "I1 = np.where(y==1)[0]\n",
    "plt.plot(X[I0,0], X[I0,1], 'bo')\n",
    "plt.plot(X[I1,0], X[I1,1], 'go')\n",
    "plt.xlabel('$x_0$', fontsize=16)\n",
    "plt.ylabel('$x_1$', fontsize=16)\n",
    "plt.subplots_adjust(bottom=0.2, left=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Simple Neural Network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the above points are not linearly separable.  We will see if we can build a simple neural network classifier to find a good decision region.  We first import some key sub-packages from `keras`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we clear the session.  This is not strictly necessary, but it is good practice as it clears any model layers that you have built before.  Otherwise, they keep hanging around."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create a very simple network with one hidden layer with `nh=4` hidden units.  There is `nout=1` output unit corresponding to the estimated class label. The `Sequential` model is what we called a \"multilayer perceptron\" in class -- it allows us to create linear stacks of fully connected layers of arbitrary width."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nin = nx  # dimension of input data\n",
    "nh = 4    # number of hidden units\n",
    "nout = 1  # number of outputs = 1 since this is binary\n",
    "model = Sequential()\n",
    "model.add(Dense(units=nh, input_shape=(nx,), activation='sigmoid', name='hidden'))\n",
    "model.add(Dense(units=nout, activation='sigmoid', name='output'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras provides a nice command for visualizing the layers.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could have created a model with more layers by simlpy called the `add` function multiple times. Each later needs to be given a seperate name. For example, try running the following block of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model2 = Sequential()\n",
    "model2.add(Dense(units=5, input_shape=(nx,), activation='sigmoid', name='hidden1'))\n",
    "model2.add(Dense(units=7, activation='sigmoid', name='hidden2'))\n",
    "model2.add(Dense(units=nout, activation='sigmoid', name='output'))\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the number of parameters for the first layer is $15$: there are two inputs which map to $5$ hidden neurons. That's $2\\cdot5$ weights + $5$ bias terms = $15$. For  the second layer there are $5\\cdot7$ weights + $7$ bias terms = $42$. \n",
    "\n",
    "For now, let's stick with using `model`, which only has one hidden layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Network\n",
    "\n",
    "To train the network, we have to select an optimizer and a loss function.  Since this is a binary classification problem, we select the `binary_crossentropy` loss.  For the optimizer, `adam` tends to works well over a wide range of problems and is a good starting point. It is a relatively simple modification of stochastic gradient descent with per-parameter adaptive learning rates. See the original paper (https://arxiv.org/abs/1412.69800 for more details.  We also set the `metrics` that we wish to track during the optimization.  In this case, we select `accuracy` on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import optimizers\n",
    "\n",
    "opt = optimizers.Adam(lr=0.01)\n",
    "model.compile(optimizer=opt,\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras provides a simple method `fit` to run the optimization.  You simply specify the number of epochs and the batch size, both discussed in class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X, y, epochs=10, batch_size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, for this problem, we need a large number of epochs -- around 1000.  We don't want to print out the progress on each epoch.  So, the code below disables the print outs by setting `verbose=0`.  Then, we run the optimization in 20 iterations with 50 epochs per iteration -- a total of 1000 epochs.  In each iteration, we use the `evaluate` method to get the loss function and accuracy and print that out manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nit = 20   # number of training iterations\n",
    "nepoch_per_it = 50  # number of epochs per iterations\n",
    "\n",
    "# Loss, accuracy and epoch per iteration\n",
    "loss = np.zeros(nit)\n",
    "acc = np.zeros(nit)\n",
    "epoch_it = np.zeros(nit)\n",
    "\n",
    "# Main iteration loop\n",
    "for it in range(nit):\n",
    "    \n",
    "    # Continue the fit of the model\n",
    "    init_epoch = it*nepoch_per_it\n",
    "    model.fit(X, y, epochs=nepoch_per_it, batch_size=100, verbose=0)\n",
    "    \n",
    "    # Measure the loss and accuracy on the training data\n",
    "    lossi, acci = model.evaluate(X,y, verbose=0)\n",
    "    epochi = (it+1)*nepoch_per_it\n",
    "    epoch_it[it] = epochi\n",
    "    loss[it] = lossi\n",
    "    acc[it] = acci\n",
    "    print(\"epoch=%4d loss=%12.4e acc=%7.5f\" % (epochi,lossi,acci))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now plot the loss function and accuracy as a function of the epoch number.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_it = np.arange(1,nit+1)*nepoch_per_it\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(epoch_it, loss)\n",
    "plt.grid()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(epoch_it, acc)\n",
    "plt.grid()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Training accuracy')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of manually measuring the loss and accuracy, we can pass a *callback* function.  This function is automatically called at each batch end.  We can then periodically print the value of the loss and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.callbacks\n",
    "\n",
    "class PeriodicPrintLoss(tensorflow.keras.callbacks.Callback):\n",
    "    def __init__(self, prt_period=100):\n",
    "        self.prt_period = prt_period\n",
    "        \n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = []\n",
    "        self.step = 0\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        # Record the loss\n",
    "        loss = logs.get('loss')\n",
    "        self.losses.append(loss)\n",
    "        \n",
    "        # Print the loss periodically\n",
    "        if (self.step % self.prt_period == 0):\n",
    "            print('step=%6d loss=%12.4e' % (self.step, loss))\n",
    "        self.step += 1\n",
    "        \n",
    "# Compute print period in steps \n",
    "batch_size = 100\n",
    "prt_period = nepoch_per_it*int(nsamp/batch_size)\n",
    "\n",
    "# Create the callback \n",
    "loss_cb = PeriodicPrintLoss(prt_period=prt_period)\n",
    "\n",
    "# Run the fit with the callback\n",
    "model.fit(X, y, epochs=200, batch_size=batch_size, verbose=0, callbacks=[loss_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Decision Regions\n",
    "\n",
    "To see how classification rule our neural network learned, we can plot the predicted class \"probability\" as a function of `(x_0,x_1)`. This is the value of the output variable before thresholding to get a class label.  To do this, we create an input matrix `Xplot` with entries that vary over `[0,1] \\times [0,1]`.  We feed that into the trained network and see what the output.  Then, we plot this like an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limits to plot the response.\n",
    "xmin = [0,0]\n",
    "xmax = [1,1]\n",
    "\n",
    "# Use meshgrid to create the 2D input\n",
    "nplot = 100\n",
    "x0plot = np.linspace(xmin[0],xmax[1],nplot)\n",
    "x1plot = np.linspace(xmin[0],xmax[1],nplot)\n",
    "x0mat, x1mat = np.meshgrid(x0plot,x1plot)\n",
    "Xplot = np.column_stack([x0mat.ravel(), x1mat.ravel()])\n",
    "\n",
    "# Compute the output \n",
    "yplot = model.predict(Xplot)\n",
    "yplot_mat = yplot[:,0].reshape((nplot, nplot))\n",
    "\n",
    "# Plot the recovered region\n",
    "plt.imshow(np.flipud(yplot_mat), extent=[xmin[0],xmax[0],xmin[0],xmax[1]], cmap=plt.cm.Reds)\n",
    "plt.colorbar()\n",
    "\n",
    "# Overlay the samples\n",
    "I0 = np.where(y==0)[0]\n",
    "I1 = np.where(y==1)[0]\n",
    "plt.plot(X[I0,0], X[I0,1], 'bo')\n",
    "plt.plot(X[I1,0], X[I1,1], 'go')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the neural network is able to learn a nonlinear classification region matching the training data points.  To understand how this nonlinear region is realized it is useful to plot the response in the each of the hidden units.  To extract the output of an intermediate layer, we create a new model, `model1` with the outputs set to the hidden layer outputs and then run the `predict` command on that model. Note that we **are not**  training this new model. We are creating it using the already trained weights/biases of `model` by using Keras' helpful `get_layer` command."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of the hidden units produces one linear decision region.  The final nonlinear region is then formed by taking a weighted combination of these regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the response in the hidden units \n",
    "layer_hid = model.get_layer('hidden')\n",
    "model1 = Model(inputs=model.input,\n",
    "               outputs=layer_hid.output)\n",
    "zhid_plot = model1.predict(Xplot)\n",
    "zhid_plot = zhid_plot.reshape((nplot,nplot,nh))\n",
    "\n",
    "# Get the weights in the output layer\n",
    "layer_out = model.get_layer('output')\n",
    "Wo, bo = layer_out.get_weights()\n",
    "\n",
    "fig = plt.figure(figsize=(10, 4))\n",
    "\n",
    "for i in range(nh):\n",
    "\n",
    "    plt.subplot(1,nh,i+1)\n",
    "    zhid_ploti = np.flipud(zhid_plot[:,:,i])\n",
    "    im = plt.imshow(zhid_ploti, extent=[xmin[0],xmax[0],xmin[0],xmax[1]], cmap=plt.cm.Reds)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.title('zh{0:d}, Wo={1:4.2f}'.format(i,Wo[i,0]))\n",
    "  \n",
    "fig.subplots_adjust(right=0.85)\n",
    "cbar_ax = fig.add_axes([0.9, 0.30, 0.05, 0.4])\n",
    "fig.colorbar(im, cax=cbar_ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let us print the model parameters. \n",
    "\n",
    "Sometimes it will be helpful to explicitly examine the weights of a trained model. The following code illustrates how to do so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g=model.get_weights()\n",
    "print('Model weights')\n",
    "print(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we print individual layer weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_hid = model.get_layer('hidden')\n",
    "Wh, bh = layer_hid.get_weights()\n",
    "print('Wh=')\n",
    "print(Wh)\n",
    "print('bh=')\n",
    "print(bh)\n",
    "\n",
    "layer_out = model.get_layer('output')\n",
    "Wo, bo = layer_out.get_weights()\n",
    "print('Wo=')\n",
    "print(Wo)\n",
    "print('bo=')\n",
    "print(bo)\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
