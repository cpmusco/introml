\documentclass[10pt]{article}
\usepackage{titlesec}
\usepackage{geometry}
\geometry{verbose,tmargin=.9in,bmargin=.9in,lmargin=1.0in,rmargin=1.0in}
\usepackage{amsmath,amsfonts,amsthm,amssymb}
\usepackage{url}
\usepackage{color}
\usepackage[usenames,dvipsnames,svgnames,table]{xcolor}
\usepackage[colorlinks=true, linkcolor=red, urlcolor=blue, citecolor=gray]{hyperref}
\usepackage{float}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{graphicx}
\usepackage{wrapfig}
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{enumerate}
\usepackage{multicol}
\usepackage{etoolbox}
\newcommand{\bs}[1]{\boldsymbol{#1}}
\newcommand{\bv}[1]{\mathbf{#1}}
\usepackage{listings}
\usepackage{bbm}

\definecolor{nyuDarkPurple}{HTML}{330662}
\definecolor{nyuOfficialPurple}{HTML}{57068c}

\newcommand{\spara}[1]{\vspace{.5em}\noindent {\large\sffamily\textcolor{nyuOfficialPurple}{#1}}}
\titleformat{\section}[hang]{\Large\sffamily\color{nyuDarkPurple}}{\thesection}{1em}{}
\titleformat{\subsection}[hang]{\large\sffamily\color{nyuDarkPurple}}{\thesection}{1em}{}
\titleformat{\subsubsection}[hang]{\normalsize\sffamily\color{gray}}{\thesection}{1em}{}

\usepackage{fancyhdr}
\pagestyle{fancy}
\lhead{\includegraphics[width=4cm]{tandon_long_color.eps}}
\rhead{\thepage}
\pagenumbering{gobble}

\setcounter{secnumdepth}{0}

% math commands
\DeclareMathOperator{\R}{\mathbb{R}}
\newcommand{\E}{\mathbb{E}}

\begin{document}
	
\begin{center}
	\normalsize
	New York University Tandon School of Engineering
	
	Computer Science and Engineering
	\medskip
	
	\large
	CS-UY 4563: Written Homework 3. 
	
	Due Wednesday, March 4th, 2020, 11:59pm.
	\medskip
	
	\normalsize 
	\noindent \emph{Collaboration is allowed on this problem set, but solutions must be written-up individually. Please list the names of any collaborators at the top of your solution set, or write ``No Collaborators" if you worked alone.}
	\medskip
\end{center} 

\subsection{Problem 1: Practice with Probabilistic Models (15pts)}
Describe a probabilistic model for each of the following data sets. There is no right answer, but your model should be reasonable and plausibly characterize the data. For each model, \textbf{separately list what parameters} which would need to be learned from past data. Also, be careful not to include in the model any data variables not explicitly listed!
\begin{enumerate}[(a)]
	\item Each data example corresponds to an apartment and has parameters: $$(x_1,x_2, y) = (\text{ZIP code}, \text{size}, \text{price}).$$ The ``ZIP code'' is for where the apartment is located, ``size'' is the apartment's square footage, and ``price'' is the current monthly rental price in dollars. 
	
		\color{blue}
		One option is to model $\text{price}$ as a linear function of size, with parameters of the linear model depending on the zip code. E.g. $\text{price} = \beta_{0,z} + \beta_{1,z}\cdot \text{size} + \eta$ where we have different parameters $\beta_{0,z}, \beta_{1,z}$ for each zip code $z$ and $\eta$ is a Gaussian random variable with mean $0$ and covariance $\sigma^2$. The zip code can be modeled as a discrete categorical random variable -- i.e. for each zip $z$ we have a probability $p(z)$ and $\sum_{z} p(z) = 1$. We set the property zip to $z$ with probability $p(z)$.  The parameters we need to learn are:
		\begin{itemize}
			\item $\beta_{0,z}, \beta_{1,z}$ for all $z$.
			\item $\sigma^2$.
			\item $p(z)$ for all $z$. 
		\end{itemize}
		\color{black}
	
	\item Each data example corresponds to a minute in the day and has parameters: $$(x,y) = (\text{time}, \text{number of riders}).$$ The ``time''  is a time of day specified in the number of minutes past midnight (e.g. 9:32am is represented as $572 = 9\times 60 + 32$) and ``number of riders'' is the current number of riders on the NYC subway system.
	
	\color{blue}
	The following is a very rough model, but it's one option: Subway ridership tends to be cyclic, with more riders in the morning and evenings, going to and from work. Model $x$ as a uniform random variable from $0, 1, \ldots, 1440$ (there are 1440 minutes in a day). Model $y$ as $y = \sin(f\cdot x + p) + \beta + \eta$ where $f,p,\beta$ are constants and $\eta$ is a Gaussian random variable with mean $0$ and covariance $\sigma^2$. The parameters we need to learn are:
	\begin{itemize}
		\item $f,p,\beta$ 
		\item $\sigma^2$.
	\end{itemize}
	\color{black}
	
	\item Each data example corresponds to a Netflix show and has parameters: $$(\vec{x}_1, x_2, y) = (\text{show description}, \text{genre}, \text{rating}).$$  The ``show description'' is a binary bag-of-words vector corresponding to the text summary of a show, ``genre'' is a category like documentary, drama, romcom, historical fiction, etc, and ``rating'' is an average numerical user rating.
	
	\color{blue}
 	Not going to write this one out fully. One option is to use a bag-of-words model like we did in class for spam, where the probability of each work depends on the genre. There are lots of ways to do ratings. For example, you could do as a Gaussian distribution with different mean for each genre. A really cool solution could also use the rating to generate the bag of words. 
	\color{black}
\end{enumerate}

\subsection{Problem 2: Gaussian Naive Bayes (15pts)}
In class it was briefly mentioned that the Naive Bayes Classifier can be extended to predictor variables with continuous values (instead of just binary variables). We will derive such an approach here

Consider a data set where each example $(\vec{x},y)$ contains a data vector $\vec{x}\in \R^d$ and a label $y\in \{0,1\}$. Each $y$ is modeled a \href{https://en.wikipedia.org/wiki/Bernoulli_distribution}{Bernoulli random variable}, which equals $1$ with probability $p$ and $0$ with probability $1-p$. To model $\vec{x}$ we have two lists of mean/variances pairs: 
\begin{align*}
&(\mu_{0,1}, \sigma_{0,1}^2), (\mu_{0,2}, \sigma_{0,2}^2), \ldots, (\mu_{0,d}, \sigma_{0,d}^2)& &\text{and}& &(\mu_{1,1}, \sigma_{1,1}^2), (\mu_{1,2}, \sigma_{1,2}^2), \ldots, (\mu_{1,d}, \sigma_{1,d}^2).
\end{align*} 
If $y$ equals $0$, then the $j^\text{th}$ entry of $\vec{x}$ is modeled as an \emph{independent} Gaussian (normal) random variable with mean $\mu_{0,j}$ and variance $\sigma_{0,j}^2$. Alternatively, if $y$ equals $1$, then the $j^\text{th}$ entry of $\vec{x}$ is modeled as an {independent}  Gaussian random variable with mean $\mu_{1,j}$ and variance $\sigma_{1,j}^2$.
\begin{enumerate}[(a)]
	\item Given a training data set $(\vec{x}_1,y_1), \ldots, (\vec{x}_n,y_n)$ write down expressions for estimating all model parameters $\mu_{i,j}$ and  $\sigma_{i,j}^2$ from the data.
	
	\color{blue}
	First let $p$ be the fraction of all data with label $1$. Next let $S_0\subseteq \{1,\ldots, n\}$ contain all indices $k$ such that $y_k = 0$ and let $S_1\subseteq 1\{1,\ldots, n\}$ contain all indices $k$ such that $y_k = 1$. 
	For each $i\in \{0,1\}$ and $j\in 1,d$, let $\mu_{i,j} = \frac{1}{|S_i|}\sum_{k \in S_i} \vec{x}_k[j]$. Then let $\sigma_{i,j} = \frac{1}{|S_i|}\sum_{k \in S_i} (\vec{x}_k[j] - \mu_{i,j})^2$.
	\color{black}
	
	\item Given a new unlabled predictor vector $\vec{x}_{new}$ we would like to predict class label ${y}_{new}$ using a \emph{maximum a posterior} (MAP) estimate. In other words, we want to choose ${y}_{new}$ to maximize the posterior probability $p({y}_{new} \mid \vec{x}_{new} )$. Write down an expression for $p({y}_{new} \mid \vec{x}_{new} )$ using Bayes Rule.
	
	\color{blue}
	$p({y}_{new} \mid \vec{x}_{new} ) = \frac{p(\vec{x}_{new}  \mid p({y}_{new} )p(y_{new})}{p(\vec{x}_{new} )}$
	\color{black}
	
	
	\item Using your result from part (b) write down a final mathematical equation (or pseudocode) for computing $p({y}_{new}=0\mid \vec{x}_{new} )$ and $p({y}_{new}=1\mid \vec{x}_{new})$. \textbf{Hint:} A correct answer should involved the PDF of a Gaussian random variable, and incorporate all model parameters $\mu_{i,j}$ and  $\sigma_{i,j}^2$.
	
	\color{blue}
	\begin{itemize}
	\item We compute $p(y) = p$ if $y = 1$ and $p(y) = 1-p$ if $y = 0$ 
	\item We compute $p(\vec{x}_{new}  \mid y) = \prod_{j=1}^d \frac{1}{\sqrt{2\pi\sigma_{y,j}^2}} e^{-(\vec{x}_{new}[j]- \mu_{y,j})^2/2\sigma_{y,j}^2} $.
	\item Finally, we compute $p(\vec{x}_{new},  \mid i) = p\cdot p(\vec{x}_{new}  \mid 1) + (1-p)\cdot p(\vec{x}_{new}  \mid 0)$.
	\item As a final step we use all these parts to evaluate the expression from part (b).
	\end{itemize}
	\color{black}
	
	
	\item How can your answer from part (c) be simplified if you only seek to compute $C\cdot p({y}_{new}\mid \vec{x}_{new} )$ for some constant $C$ you choose? What if you only seek to compute $B\cdot\log\left(C\cdot p({y}_{new} \mid \vec{x}_{new} )\right)$ for some constants $B,C$ you choose?
	Can either or both of these simplified expression be used in deciding on the MAP estimate for $y_{new}$?
	
	\color{blue}
	\begin{itemize}
		\item For the first part, we have: $C p({y}_{new} \mid \vec{x}_{new} ) = p(y)\prod_{j=1}^d \frac{1}{\sigma_{y,j}} e^{-(\vec{x}_{new}[j]- \mu_{y,j})^2/2\sigma_{y,j}^2}$.
		\item For the second, we have $B\cdot\ln\left(C\cdot p({y}_{new}=0\mid \vec{x}_{new} )\right) = \log(p(y)) - \sum_{j=1}^d \log(\sigma_{y,j}) - \sum_{i=1}^d (\vec{x}_{new}[j]- \mu_{y,j})^2/2\sigma_{y,j}^2$
	\end{itemize}
Both can be used since $Cx$ and $B\log(Cx)$ are monotonic (order preserving) functions as long as $C$ and $B$ are positive.
	\color{black}
	
\end{enumerate}

\newpage
\subsection{Problem 3: Bayesian Central Tendency (10pts)}
Let's revisit Question 3 on Written Homework 1 from a Bayesian perspective. This was the question about loss functions for measures of central tendency. 
\begin{enumerate}[(a)]
	\item Suppose we have a data set of scalar numbers $x_1, \ldots, x_n$. Assume a Bayesian probabilistic model in which the numbers are drawn from a Gaussian distribution with unknown mean $\mu$ and variance $\sigma^2$. We have no prior information on $\mu$ and  $\sigma^2$: we assume all parameters are equally likely. Prove that the sample mean $\hat{\mu} = \frac{1}{n}\sum_{i=1}^n x_i$ is a MAP estimate for the unknown parameter $\mu$.
	
	\color{blue}
		Using Bayes rule, we have that $p(\mu \mid x) \sim p(x\mid \mu)$, since we assume a uniform prior on $\mu$. Then $p(x\mid \mu) \sim \prod_{i=1}^n e^{-(x_i-\mu)^2/\sigma^2}$ and $\log(p(x\mid \mu)) \sim \sum_{i=1}^n -(x_i-\mu)^2$. So the MAP estimate is the $\mu$ which maximizes $\sum_{i=1}^n -(x_i-\mu)^2$, which is the same as minimizing $\sum_{i=1}^n (x_i-\mu)^2$. In Homework 1 we already proved that the mean minimizes this expression.
	\color{black}
	
	\item Now assume a Bayesian probabilistic model in which the numbers are drawn from a \href{https://en.wikipedia.org/wiki/Laplace_distribution}{Laplace Distribution} with unknown mean $\mu$ and variance $2b^2$. Prove that the sample median is a MAP estimate for the unknown parameter $\mu$. \textbf{Hint:} Look back at Homework 1.
	
	\color{blue}
	Using Bayes rule, we have that $p(\mu \mid x) \sim p(x\mid \mu)$, since we assume a uniform prior on $\mu$. Then $p(x\mid \mu) \sim \prod_{i=1}^n e^{-|x_i-\mu|/b}$ and $\log(p(x\mid \mu)) \sim \sum_{i=1}^n -|x_i-\mu|^2$. So the MAP estimate is the $\mu$ which minimizes $\sum_{i=1}^n |x_i-\mu|$, which we already proved to be the median in Homework 1. 
	\color{black}
	
	\item (\textbf{Extra Credit -- 5pt}) Assume a Bayesian probabilistic model in which the numbers are drawn from a uniform distribution centered at $\mu$ and of width $2b$. I.e. each $x_i$ is drawn uniformly from the interval $[\mu-b, \mu + b]$. Further assume that $b$ itself is modeled as a Gaussian random variable with mean 0 and variance 1. So smaller values of $b$ are more likely. What is a MAP estimate for $\mu$?
	
	\color{blue}
	The MAP estimate is $\frac{\min_i(x_i) + \max_i(x_i)}{2}$, which is called the midrange. We already saw this estimate on Homework 1. 
	\color{black}
\end{enumerate}

\end{document}