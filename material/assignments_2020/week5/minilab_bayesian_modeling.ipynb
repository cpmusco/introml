{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Classification via Probabilistic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, we will practice working with Bayesian probabilistic modeling.\n",
    "\n",
    "You will implement a probabilistic model of your own design from Homework 3, and you will also use the generalization of Naive Bayes Classification for classification on the breast cancer dataset from our logistic regression demo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Warm Up: Implementing a Probabilisitc Model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the probabilistic model you came up with for Problem 1, part (a) on written homework 3. Limit your model to only include data for any **three zipcodes** of your choice in New York City.\n",
    "\n",
    "You will probably want to use one or more functions in the `numpy.random` package.\n",
    "\n",
    "Feel free to simply select reasonable parameters for your model using your own intuition, but **bonus points will be given if you use outside data to inform the model.** Please include a link to any outside sources and explain how the data was used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_rent_data(n):\n",
    "    \"\"\"\n",
    "    Randomly generate a synthetic dataset of apartment rental data with n examples.\n",
    "    Return: X, a n x 2 numpy array, where the first column contains a zip code and\n",
    "    the second contains a square footage number.\n",
    "    y, an n x 1 numpy array containing a monthly rental price\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Generate 1000 data examples using the probabilistic model implemented in `generate_rent_data(n)`. \n",
    "* Plot the data on a scatter plot, with the x-axis being apartment size and the y-axis being rent. Color each data points to indicate which of the three zip codes it is from. \n",
    "\n",
    "Confirm that the data looks reasonable for the zip codes you selected! I have provided an example result below:\n",
    "\n",
    "<img src=\"sample_output.png\" width=\"400\">\n",
    "\n",
    "Your data will look different depending on how you designed your probabilistic model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "X,y = generate_rent_data(1000)\n",
    "# plt.plot(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Breast Cancer Diagnosis via Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this portion of the lab, we will revisit the Breast Cancer Diagnosis problem from `demo_breast_cancer.ipynb` and try to classifiy examples using the Gaussian Naive Bayes method developed in Problem 2 of Homework 3.\n",
    "\n",
    "As a reminder, the data set is described here:\n",
    "https://archive.ics.uci.edu/ml/datasets/breast+cancer+wisconsin+(original) \n",
    "\n",
    "More information on the problem can be found in the demo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and Visualizing the Data\n",
    "\n",
    "We first load the packages and data as in the demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn import datasets, linear_model, preprocessing\n",
    "%matplotlib inline\n",
    "names = ['id','thick','size_unif','shape_unif','marg','cell_size','bare',\n",
    "         'chrom','normal','mit','class']\n",
    "df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/' +\n",
    "                 'breast-cancer-wisconsin/breast-cancer-wisconsin.data',\n",
    "                names=names,na_values='?',header=None)\n",
    "df = df.dropna()\n",
    "\n",
    "# Get the response.  Convert to a zero-one indicator \n",
    "yraw = np.array(df['class'])\n",
    "BEN_VAL = 2   # value in the 'class' label for benign samples\n",
    "MAL_VAL = 4   # value in the 'class' label for malignant samples\n",
    "y = (yraw == MAL_VAL).astype(int) # now y has values of 0,1 \n",
    "Iben = (y==0)\n",
    "Imal = (y==1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this lab we are going to use all predictors we have at our disposal, as was done at the end of the demo. The code below places all of the predictor data in a matrix `Xfull` of dimension (n x d). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xnames = names[:-1]\n",
    "Xfull = np.array(df[xnames])\n",
    "n = Xfull.shape[0]\n",
    "d = Xfull.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step in using the Gaussian Naive Bayes method is to estimate all parameters of the probabilistic model. These include:\n",
    "* `p` -- the probability that a data example is malignant (label 1)\n",
    "* `mub` = `[mub[0], \\ldots, mub[d-1]]` -- the expected value of each predictor variable for benign examples.\n",
    "* `sigb` = `[sigb[0], \\ldots, sigb[d-1]]` -- the variance value of each predictor variable for benign examples.\n",
    "* `mum` = `[mum[0], \\ldots, mum[d-1]]` -- the expected value of each predictor variable for malignant examples.\n",
    "* `sigm` = `[sigm[0], \\ldots, sigm[d-1]]` -- the variance value of each predictor variable for maglignant examples.\n",
    "\n",
    "Compute estimates for these values below using teh data in `Xfull`. \n",
    "\n",
    "**Hint**: For a compact approach, you might want to use the boolean arrays `Imal` and `Iben` created above for \"mask indexing\" (see [these docs](https://docs.scipy.org/doc/numpy/user/basics.indexing.html))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# p = ...\n",
    "# mub = ..\n",
    "# sigb = ...\n",
    "# mum = ..\n",
    "# sigm = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, for every row $\\vec{x}$ in `Xfull`, use the model parameters determined above to compute the maximum a posterior (MAP) estimate for the class label $y$. You should use the equations derived in your solution to Problem 3 (b) on the homework. \n",
    "\n",
    "You may need to be thoughtful in how you do this computation to avoid numerical underflow or overflow in your computations: keep in mind that you just need to determine which of $p(y=0 | \\vec{x})$ or $p(y=1 | \\vec{x})$ is **larger** -- you don't necessarily need to compute both values explicitly. \n",
    "\n",
    "Store your MAP estimates for the data examples in an integer vector `yhat` (with 0 indicating benign, 1 indicating malignant)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# ...\n",
    "# yhat = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the accuracy of your estimates using the code below. You should see a result which is very competitive with logistic regression, which is pretty cool given how simple this algorithm is!\n",
    "\n",
    "**Note**: we would ideally want to do a proper train/test split to evaluate the performance of both logistic regression and Naive Bayes classification. We're just looking at training set loss to keep things simple, and because the number of features is relatively small, so we're not super worried about overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = np.mean(yhat == y)\n",
    "print(\"Accuracy on training data = %f\" % acc)\n",
    "recall = np.sum((yhat == 1)*(y == 1))/np.sum(y == 1)\n",
    "precision = np.sum((yhat == 1)*(y == 1))/np.sum(yhat == 1)\n",
    "print(\"Recall: \" + str(recall))\n",
    "print(\"Precision: \" + str(precision))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
