{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo:  Predicting Diabetes Progression using Mulitple Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this demo, you will learn how to:\n",
    "* Fit multiple linear regression models, by hand and using Python's Scikit-Learn package.  \n",
    "* Split data into training and test set.\n",
    "* Manipulate and visualize multivariable arrays.\n",
    "\n",
    "We first load the packages as usual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also load a few modules from [scikit-learn](https://scikit-learn.org/stable/), which is one of the most popular machine learning libraries in Python that does not focus specifically on neural nets and deep learning. Using the libary is a bit overkill at this point, but I want you to get some practice because it will be very useful later in the course, and for your projects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets, linear_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diabetes Data Example\n",
    "To illustrate the concepts, we load a well-known diabetes data set.  This dataset can be downloaded in raw form from [https://www4.stat.ncsu.edu/~boos/var.select/diabetes.html](https://www4.stat.ncsu.edu/~boos/var.select/diabetes.html). However, it is also included in the `sklearn.datasets` module, which prepackages a number of benchmark datasets for easy experimentation. It can be loaded as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the diabetes dataset\n",
    "diabetes = datasets.load_diabetes()\n",
    "X = diabetes.data\n",
    "y = diabetes.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's always good to check what type of objects we have loaded in. You can confirm using the `type` function that our data is preloaded into `numpy` arrays, which is how we want it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predictor variables are **age**, **sex**, **body mass index**, **average blood pressure**, and **six blood serum measurements** (think iron, cholesterol, lipid levels, etc.). Each corresponds to a column in the data matrix `X`. Note that these columns have been *mean centered* and had their *variance normalized* (i.e. scaled). This will not effect your linear regression model at all, but it's good to know."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you look at the first 5 entries of the **age** column, we don't see age numbers we might expect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "str(X[0:5,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And when you look at the first 5 entries of the **sex** column, we don't see binary categories like 1 for female, 2 for male. We see real numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "str(X[0:5,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**: This is one reason to be cautious about using built in datasets. Often a lot of information is lost during preprocessing which makes it harder to reason about the data than if you were working with the raw files. This is just a simple teaching demo, so we don't care for today, but keep that in mind when downloading data for your projects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target values `y` represent a \"quantitative measure of diabetes disease progression\" that we wish to predict. The exact meaning of this measure seems to have been lost in data translation, but we will work with it anyway.\n",
    "\n",
    "The number of attributes and samples are computed from the shape:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nsamp, natt = X.shape\n",
    "print(\"num samples=\"+str(nsamp)+\" num attributes=\"+str(natt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Simple Linear Regression for Each Feature Individually\n",
    "\n",
    "As a first attempt to predict diabetes progression, we could try *one attribute at a time*.  That is, for each attribute $x_k$, we could attempt to fit a simple linear regression model:\n",
    "$$ y \\approx \\beta_{0,k} + \\beta_{1,k}x_k$$\n",
    "where $\\beta_{0,k}$ and $\\beta_{1,k}$ are the coefficients in the simple linear regression model using only the attribute $x_k$.\n",
    "\n",
    "The following code computes the $\\ell_2$ loss for each variable $k$ as well as the coefficients in the linear model, $\\beta_{0,k}$ and $\\beta_{1,k}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ym = np.mean(y)\n",
    "losses = np.zeros(natt)\n",
    "beta0 = np.zeros(natt)\n",
    "beta1 = np.zeros(natt)\n",
    "for k in range(natt):\n",
    "    xm = np.mean(X[:,k])\n",
    "    sxy = np.mean((X[:,k]-xm)*(y-ym))\n",
    "    sxx = np.mean((X[:,k]-xm)**2)\n",
    "    beta1[k] = sxy/sxx\n",
    "    beta0[k] = ym - beta1[k]*xm\n",
    "    errs = y - beta1[k]*X[:,k] - beta0[k]\n",
    "    losses[k] = np.sum(errs**2)\n",
    "    \n",
    "    print(str(k)+\" loss=\"+\"{:.2e}\".format(losses[k])+\" beta0=\"+str(beta0[k])+\" beta1=\"+str(beta1[k]))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test yourself:** Why are all the `beta0` values the same, no matter what predictor we use?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the best prediction is given by BMI index, with loss 1.72e+06. If you convert this to an $R^2$ value, as described in class, we get a value of $R^2 = .344$, which on a scale from $[0,1]$ does not indicate too strong of a fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syy = np.mean((y-ym)**2)\n",
    "rsqr = 1 - np.min(losses)/(nsamp*syy)\n",
    "print(\"Rsqr=\"+\"{:.3}\".format(rsqr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see this somewhat poor fit in a scatter plot as well where there is a significat variation from the regression line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the index of the single variable with the best squared loss\n",
    "imax = np.argmin(losses)\n",
    "\n",
    "# Regression line over the range of x values\n",
    "xmin = np.min(X[:,imax])\n",
    "xmax = np.max(X[:,imax])\n",
    "ymin = beta0[imax] + beta1[imax]*xmin\n",
    "ymax = beta0[imax] + beta1[imax]*xmax\n",
    "plt.plot([xmin,xmax], [ymin,ymax], 'r-', linewidth=3)\n",
    "\n",
    "# Scatter plot of points\n",
    "plt.scatter(X[:,imax],y)\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the calculations above could have been done without a for-loop using Python broadcasting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the means\n",
    "ym = np.mean(y) \n",
    "y1 = y-ym  # a column vecotor each minus mean\n",
    "Xm = np.mean(X,axis=0) # averaging over column, resulting a row vector of dimension natt\n",
    "X1 = X - Xm[None,:] # minus the same mean in each column\n",
    "\n",
    "# Compute the correlations per features\n",
    "Sxx = np.mean(X1**2,axis=0) #a row vector with each element indicating the variance of one attribute\n",
    "Sxy = np.mean(X1*y1[:,None],axis=0) #a row vector with each element indicating the covarance on one attribute to the targer\n",
    "\n",
    "# Compute the coefficients and losses per feature\n",
    "beta1 = Sxy/Sxx # element wise division, resulting a row vector containing  beta1 for each attribute\n",
    "beta0 = ym - beta1*Xm # element wise multiplication, resulting a row vector containing beta0 for each attribute\n",
    "errs = ((X*beta1) + beta0) - y[:,None] # results in a matrix where every column contains the residuals for predictor k\n",
    "losses = np.sum(errs**2,axis=0) # sums up the squared size of the residuals across each column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us see whether we get the same result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for k in range(natt):\n",
    "    print(str(k)+\" loss=\"+\"{:.2e}\".format(losses[k])+\" beta0=\"+str(beta0[k])+\" beta1=\"+str(beta1[k]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improvements with a Multiple Variable Linear Model\n",
    "\n",
    "One possible way to try to improve the fit is to use multiple variables at the same time. \n",
    "\n",
    "We can manually compute the regression coefficients using the least-squares matrix formula given in class. \n",
    "\n",
    "To compute the coefficients manually, we first append an all ones columns onto `X` using the `ones` command and `hstack`.  Note that after we do this, `X` will have 11 columns -- one more column that original data matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ones = np.ones((nsamp,1))\n",
    "X_orig = X\n",
    "X = np.hstack((ones,X_orig))\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to solve $\\min_{\\mathbf{\\beta}}\\|\\mathbf{y} - \\mathbf{X\\beta}\\|_2^2$. The most obvious way to do so is via the direct matrix equation derived in class: $\\mathbf{\\beta}^* = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xt = np.transpose(X)\n",
    "beta=np.dot(np.linalg.inv(np.dot(Xt,X)),np.dot(Xt,y))\n",
    "print(beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `beta[0]` is the value of the intercept from the regression fit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The notation looks a bit cleaner if we use `numpy`'s `matrix` type. Note that the `matrix` type is a subclass of `ndarray`. It really has no advantages other than the fact that the multiplication operator `*` performs actual matrix multiplication. Seems like a small thing, but can save you a lot of headache over using multiple nested `np.dot` operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Xmat=np.matrix(X)\n",
    "ymat=np.matrix(y)\n",
    "ymat=np.transpose(ymat)\n",
    "Xmatt=np.transpose(Xmat)\n",
    "beta=np.linalg.inv(Xmatt*Xmat)*Xmatt*ymat\n",
    "print(beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An often slightly faster alternative is to use the `lstsq` method, which solves $\\min_{\\mathbf{\\beta}}\\|\\mathbf{y} - \\mathbf{X\\beta}\\|_2^2$ using a QR decomposition. This should find the exact same least-squares fit. You might get a warning when you run `lstqr` about some defaults changing in the future. You can safetly ignore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "out = np.linalg.lstsq(X,y)\n",
    "beta = out[0]\n",
    "print(beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, for very large problems (many examples and/or features), *iterative* methods can offer a much faster approach to solving $\\min_{\\mathbf{\\beta}}\\|\\mathbf{y} - \\mathbf{X\\beta}\\|_2^2$. There are several options avialable in the `scipy` library, including the populat LSQR method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = sp.sparse.linalg.lsqr(X,y)\n",
    "beta = out[0]\n",
    "print(beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully you got the same values of $\\mathbf{\\beta}$ no matter which algorithm you used. The differences are negligble because this demo problem is so small, but if you ever want to compare runtimes head-to-head, you can use the `time` library to time different operations. Here's an example that times the direct approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "t = time.time()\n",
    "beta=np.linalg.inv(Xmatt*Xmat)*Xmatt*ymat\n",
    "elapsed = time.time() - t\n",
    "print(\"direct matrix computation time: \" + str(elapsed) + \" seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compute the loss for our muliple variable model. Be careful with arrays vs. matrices! Always check what you're working with by running commands like `type(beta)`. Or multiplications may not behave as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "yp = Xmat*beta\n",
    "errs = np.array(ymat - yp)\n",
    "lossm = np.sum(errs**2)\n",
    "\n",
    "print(\"multiple variable loss=\"+\"{:.2e}\".format(lossm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could have also done everything using matrix operations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yp = Xmat*beta\n",
    "lossm = np.linalg.norm(np.array(ymat - yp))**2\n",
    "\n",
    "print(\"multiple variable loss=\"+\"{:.2e}\".format(lossm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this loss is about $30\\%$ lower than what we acheived with the best single variable linear regression, which is significant. Let's also normalize the loss to get an $R^2$ value, which we can compare to the value of $34\\%$ we obtained earlier. Higher $R^2$ is better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rsqr = 1 - lossm/(nsamp*syy)\n",
    "rsqr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using built in sci-kit learn model\n",
    "\n",
    "We can also fit the multiple variable linear regression model using Sci-kit Learn's built in functionality. As mentioned, this is a bit overkill at this point, but it's good to see because you might use other built in models from `sklearn` later in the course. The disadvantage is that this approach leaves everything inside a \"black-box\". For example, how do you think this method is solving the required least-squares optimization problem? Using an iterative method like `lsqr`? Or a direct method?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To fit the linear model, we first create a regression object and then fit the data with regression object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X[:,1:11]\n",
    "regr = linear_model.LinearRegression()\n",
    "regr.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the intercept and other coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "regr.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We next compute the squared loss by using the `predict` function and you can verify that it's the same as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = regr.predict(X)\n",
    "lossm = np.linalg.norm(y_pred - y)**2\n",
    "\n",
    "print(\"multiple variable loss=\"+\"{:.2e}\".format(lossm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
