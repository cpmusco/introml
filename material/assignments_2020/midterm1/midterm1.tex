\documentclass[10pt]{article}
\usepackage{titlesec}
\usepackage{geometry}
\geometry{verbose,tmargin=.9in,bmargin=.9in,lmargin=1.0in,rmargin=1.0in}
\usepackage{amsmath,amsfonts,amsthm,amssymb}
\usepackage{url}
\usepackage{color}
\usepackage[usenames,dvipsnames,svgnames,table]{xcolor}
\usepackage[colorlinks=true, linkcolor=red, urlcolor=blue, citecolor=gray]{hyperref}
\usepackage{float}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{graphicx}
\usepackage{wrapfig}
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{enumitem}
\usepackage{multicol}
\usepackage{etoolbox}

\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}

\definecolor{nyuDarkPurple}{HTML}{330662}
\definecolor{nyuOfficialPurple}{HTML}{57068c}

\newcommand{\spara}[1]{\vspace{.5em}\noindent {\large\sffamily\textcolor{nyuOfficialPurple}{#1}}}
\titleformat{\section}[hang]{\Large\sffamily\color{nyuDarkPurple}}{\thesection}{1em}{}
\titleformat{\subsection}[hang]{\large\sffamily\color{nyuDarkPurple}}{\thesection}{1em}{}
\titleformat{\subsubsection}[hang]{\normalsize\sffamily\color{gray}}{\thesection}{1em}{}

\usepackage{fancyhdr}
\pagestyle{fancy}
\lhead{\includegraphics[width=4cm]{tandon_long_color.eps}}
\rhead{\thepage}
\pagenumbering{gobble}

\setcounter{secnumdepth}{0}

% math commands
\DeclareMathOperator{\R}{\mathbb{R}}
\newcommand{\E}{\mathbb{E}}

\begin{document}
	
\begin{center}
	\normalsize
	New York University Tandon School of Engineering
	
	Computer Science and Engineering
	\medskip
	
	\large
	CS-UY 4563: Midterm Exam 1. 
	
	Monday, Mar. 9th, 2020, 9:00 - 10:15pm
	
	50 Total Points
	\medskip
\end{center} 

\subsection{Directions}
\begin{itemize}
	\item Show all of your work to receive full (and partial) credit.
	\item If more space is required, you may use extra sheets of paper clearly marked with your name, netid, and the problem you are working on.
\end{itemize}

\subsection{1. Always, Sometimes, Never. (\textbf{\small 12pts -- 3pts each})} 
Indicate whether each of the following statements is ALWAYS true, SOMETIMES true, or NEVER true. \textbf{No justification is necessary to receive full credit for a correct answer}. To earn partial credit if you are wrong, you may provide a short justification or example to explain your choice.
\begin{enumerate}[label=(\alph*)]
	\item The empirical risk of a model is lower than the population risk.
	
	ALWAYS\hspace{1em} SOMETIMES\hspace{1em} NEVER\vspace{4.5em}
	
	\item You train a multiple linear regression model with varying levels of $\ell_2$ regularization. Let $\vec{\beta}^{(1)} = \argmin_{\vec{\beta}}  \|X\vec{\beta} - \vec{y}\|_2^2 + \lambda_1 \|\vec{\beta}\|_2^2$ and let $\vec{\beta}^{(2)} = \argmin_{\vec{\beta}}  \|X\vec{\beta} - \vec{y}\|_2^2 + \lambda_2 \|\vec{\beta}\|_2^2$. 
	
	If $\lambda_1 > \lambda_2$, is $\|X\vec{\beta}^{(1)}  - \vec{y}\|_2^2 < \|X\vec{\beta}^{(2)}  - \vec{y}\|_2^2$?

	ALWAYS\hspace{1em} SOMETIMES\hspace{1em} NEVER\vspace{4.5em}
	
	\item The linear classifier found by logistic regression minimizes error rate ( $0$-$1$ loss)  on the training data. 
	
	ALWAYS\hspace{1em} SOMETIMES\hspace{1em} NEVER\vspace{4.5em}
	
%	\item For convex functions $f(x)$ and $g(x)$, $f(x)\cdot g(x)$ is convex.
%	
%	ALWAYS\hspace{1em} SOMETIMES\hspace{1em} NEVER\vspace{4em}
	
	\item Consider a multiple linear regression problem where each data example has the form $(\vec{x},y) = ([x_1,x_2],y)$. Transform the predictor variables by adding quadratic terms, so each new data example has the form $(\vec{x}_{trans},y)= ([x_1,x_2,x_1^2, x_2^2, x_1x_2],y)$. Let $L^*$ be the minimum training loss for the original problem and let ${L}_{trans}^*$ be the minimum training loss for the transformed problem. Is ${L}_{trans}^* \leq L^*$?
	
	ALWAYS\hspace{1em} SOMETIMES\hspace{1em} NEVER\vspace{4.5em}
	

\end{enumerate}

\subsection{2. Model Diagnosis Short Answer (\textbf{\small 8pts})}
You are trying to solve a prediction problem using a multiple linear regression model with $\ell_2$ loss. 
You first split the data set into a train set (80\%) and a test set (20\%). You then train the model on the train set to obtain a parameter vector $\vec{\beta}$. Using $\vec{\beta}$, you evaluate the average squared loss of the regression model on the train and test set, separately.


For each of the following scenarios, circle all answers that apply.  \textbf{No justification is necessary to receive full credit for a correct answer}. To earn partial credit if you are wrong, you may provide a short justification.
\begin{enumerate}[label=(\alph*)]
	\item  (4pts) The average squared loss on the train set is $1.5$ and the average squared loss on the test set is $12.6$. \textbf{Which of the following techniques is likely to improve your average test loss?} 
	
	\vspace{1em}REGULARIZATION\hspace{1em} FEATURE SELECTION\hspace{1em} FEATURE TRANSFORM \hspace{1em} DATA SCALING\vspace{8em}

	
	\item (4pts) The average squared loss on the train set is $10.2$ and the average squared loss on the test set is $9.9$. \textbf{Which of the following techniques is likely to improve your average test loss?}
	
	\vspace{1em}REGULARIZATION\hspace{1em} FEATURE SELECTION\hspace{1em} FEATURE TRANSFORM \hspace{1em} DATA SCALING\vspace{8em}
	
	

\end{enumerate}




\newpage
\subsection{3. Model Diagnosis 2 (\textbf{\small 10pts})}
Consider the following scatter plots of data for three binary classification problems. $x_1$ and $x_2$ are the independent variables and class labels are indicated by points with a different shape and shade.

\includegraphics[width=\textwidth]{examp_datasets.png}



\begin{enumerate}[label=(\alph*)]
	\item (4pts) Indicate which of the three clustering problems could be solved to high accuracy (small error rate) using a logistic regression model with no regularization and no feature transformations. 
	\vspace{10em}
	
	\item (6pts) For any of the problems that you believe are not \emph{directly solvable} with logistic regression, suggest a possible feature transformation which \emph{would make it possible} to obtain a high accuracy solution with logistic regression. For each problem, your solution should be a set of new features $\phi_1(x_1,x_2), \phi_2(x_1,x_2), \ldots, \phi_q(x_1,x_2)$ that depend on the original features $x_1$ and $x_2$. You may use as large a $q$ as you need.
	\vspace{12em}


\end{enumerate}


\vspace{10em}

\newpage
\subsection{4. Loss Minimization. (\textbf{\small 10pts})}

For data with one predictor and one target: $(x_1, y_1), \ldots, (x_n, y_n)$, consider a linear regression model:

\begin{align*}
f_{\beta_0,\beta_1}(x) = \beta_0 + \beta_1 x
\end{align*}
with \emph{exponential loss}:
\begin{align*}
L(\beta_0,\beta_1) = \sum_{i=1}^n e^{(y_i - f_{\beta_0,\beta_1}(x_i))^2}
\end{align*}

\begin{enumerate}[label=(\alph*)]
	\item (5pts) Write down an expression for the gradient of the loss $L$.
	\vspace{16em}
	\item (2pts) Name two algorithms/methods which could be used to minimize $L$.
	\vspace{12em}
	\item (3pts) In general, is this exponential loss more or less robust to outliers when compared to $\ell_2$ loss? How about when compared to $\ell_\infty$ loss? 
\end{enumerate}


\newpage

\subsection{5. Bayesian Crab Classification (\textbf{\small 10pts})}
A biologist is collecting specimens from two species of crabs, species $S_0$ and $S_1$. These species live in the same habitat and look similar to the human eye. To accelerate crab sorting by species, the biologist wants to develop a simple classification rule based on body measurements. She observes that the ratio of \emph{forehead breadth} to overall \emph{body length} differs between crabs in species $S_0$ and $S_1$. The biologist proposes to measure this ratio (denoted by $R$) and use it as a single predictor variable for classification.

The biologist assumes that the crab data comes from a ``mixture of Gaussians'' probabilistic model. In particular, she assumes that for each species, $R$ follows a normal (Gaussian) probability distribution, with different parameters for each species. The biologist makes the following concrete observations:
\begin{itemize}
	\item $35\%$ of all crabs collected belong to $S_0$ and the remaining $65\%$ belong to $S_1$. 
	\item For crabs in $S_0$, the average value of $R$ is $.5$. For crabs in $S_1$, the average value of $R$ is $.4$. 
	\item For both species, the standard deviation of $R$ is $.1$.
\end{itemize}


\begin{enumerate}[label=(\alph*)]
	\item (6pts) Suppose we collect a new crab with forehead breadth to body length ratio $R_{new}$.  The biologist would like to assign this crab to $S_0$ or $S_1$ using a maximum a posterior (MAP) classification rule. Denote this rule by $f: \R\rightarrow \{S_0,S_1\}$. The rule takes as input the ratio $R_{new}$ and outputs $S_0$ or $S_1$.
	
	Write down all mathematical expressions that would need to be evaluated to compute $f$ for a given input $R_{new}$. Your expressions do not need to be simplified, but they should not involve unknown variables besides $R_{new}$. \textbf{Hint:} Use Bayes rule.
	
	\vspace{16em}
	
	\item (4pts) Show that, for this problem, the classification rule $f$ has the following form:
	\begin{align*}
	f(R_{new}) = \begin{cases}
	S_0 \text{ if } R_{new} \geq \lambda \\
	S_1 \text{ if } R_{new} < \lambda,
	\end{cases}
	\end{align*}
	for some fixed threshold parameter $\lambda$ (you do not need to explicitly compute $\lambda$).
	\vspace{12em}
	
	\item (3pts -- extra credit) Given the biologist's data above, will the threshold $\lambda$ for the MAP classification rule be EQUAL TO, LARGER, or SMALLER than $.45$? Justify your answer in a sentence or two. This problem can be solved without a calculator. 
	
\end{enumerate}

\end{document}