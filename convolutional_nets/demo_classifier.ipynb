{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo:  CNN Classifier for CIFAR10\n",
    "\n",
    "In this demo, you will learn to:\n",
    "\n",
    "* Load the classic CIFAR10 dataset from keras and visualize the images\n",
    "* Train and test a simple CNN classifier for the dataset\n",
    "* Enhance the classifiers with batch normalization, dropout and data augmentation and evaluate the relative performance gains.\n",
    "\n",
    "\n",
    "The [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html) is a collection of 60,000 color, `32x32`-pixel images in ten classes. Classes include common objects such as airplanes, automobiles, birds, cats and so on. There are 50,000 train and 10,000 test images. Keras can automatically download the dataset from `keras.datasets`. Note that it will take some time to downloading the dataset for the first time. \n",
    "\n",
    "State of the art results are achieved using very large Convolutional Neural networks. Model performance is reported in this [classification accuracy table](http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#43494641522d3130), with state-of-the-art results at 96.5%. Note that for this problem human performance is roughly 94%.  In this demo, we will use a very basic/shallow CNN.  With suitable enhancements we can performance ~78%.\n",
    "\n",
    "Most of the code and the description in this demo is taken from `cifar10_cnn.py` available at [keras-team Github page](https://github.com/keras-team/keras/blob/master/examples/cifar10_cnn.py) as well as from the tutorial page by [Jason Brownlee](https://machinelearningmastery.com/object-recognition-convolutional-neural-networks-keras-deep-learning-library/).  Also, thanks to [Phil Schniter](http://www2.ece.ohio-state.edu/~schniter/) for helping adjust some parameters.\n",
    "\n",
    "\n",
    "## Loading Basic Packages\n",
    "\n",
    "We first load some basic packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing Environment\n",
    "\n",
    "This demo will be **very slow** unless a GPU is used.  The function below is one way to test if your installation of Tensorflow has access to a GPU. There are many reasons it may not: for example, Tensorflow does not support GPUs at all on Macs. \n",
    "\n",
    "If you do not have access to a GPU on your local machine, we highly recommend running this lab through [Google Colab](https://colab.research.google.com/). To make sure Colab is using a GPU, click on the Runtime tab and then Change Runtime Environment. Select GPU under hardware acceleration. Colab even has access to Google's special purpose [Tensor Processing Units](https://en.wikipedia.org/wiki/Tensor_processing_unit) but we found these to be significantly slower than using the standard GPU acceleration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a 3×3 plot of photographs. The images have been scaled up from their small 32×32 size, but you can clearly see trucks horses and cars. You can also see some distortion in some images that have been forced to the square aspect ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "(Xtr,ytr), (Xts,yts) = cifar10.load_data()\n",
    "ntr, nrow, ncol, nchan = Xtr.shape\n",
    "nts = Xts.shape[0]\n",
    "\n",
    "print('Xtr shape:  ' + str(Xtr.shape))\n",
    "print('Xts shape:  ' + str(Xts.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plt_image(im):\n",
    "    plt.imshow(im)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    \n",
    "# Print a few random samples\n",
    "nplot = 9\n",
    "I = np.random.permutation(ntr)\n",
    "for i in range(0, 9):\n",
    "    plt.subplot(3,3,i+1)\n",
    "    plt_image(Xtr[I[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import some more packages for building our CNN model and saving the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.models import load_model #save and load models\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D, BatchNormalization\n",
    "from tensorflow.keras import optimizers\n",
    "import tensorflow.keras.backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pixel values are in the range of 0 to 255 for each of the red, green and blue channels.\n",
    "\n",
    "It is good practice to work with normalized data. Because the input values are well understood, we can easily normalize to the range 0 to 1 by dividing each value by the maximum observation which is 255.  Note, the data is loaded as integers, so we must cast it to floating point values in order to perform the division."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtr = Xtr.astype('float32') / 255.\n",
    "Xts = Xts.astype('float32') / 255."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Different Classifiers\n",
    "\n",
    "We now define a function to create a model.  The function has two paramters:\n",
    "\n",
    "* `use_bn`:  Adds BatchNormalization.\n",
    "* `use_dropout`:  Adds Dropout.\n",
    "\n",
    "By setting the parameters, we can experiment with different model features and compare their performance.  The model has two convolutional layers + two FC layers.  Dropout, if added, is done on the FC layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mod(use_dropout=False, use_bn=False):\n",
    "    num_classes = 10\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), \n",
    "                     padding='valid', activation='relu',\n",
    "                     input_shape=Xtr.shape[1:]))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    if use_bn:\n",
    "        model.add(BatchNormalization())\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Flatten())\n",
    "    if use_bn:\n",
    "        model.add(BatchNormalization())\n",
    "    if use_dropout:\n",
    "        model.add(Dropout(0.5))\n",
    "    model.add(Dense(512, activation = 'relu'))\n",
    "    if use_bn:\n",
    "        model.add(BatchNormalization())\n",
    "    if use_dropout:\n",
    "        model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To study data augmentation, we also create an `ImageDataGenerator` object that will create augmented images for the training data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_datagen():\n",
    "    datagen = ImageDataGenerator(\n",
    "        rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        # randomly shift images horizontally (fraction of total width)\n",
    "        width_shift_range=0.05,\n",
    "        # randomly shift images vertically (fraction of total height)\n",
    "        height_shift_range=0.05,\n",
    "        horizontal_flip=True,  # randomly flip images\n",
    "        # image data format, either \"channels_first\" or \"channels_last\"\n",
    "        data_format=\"channels_last\")\n",
    "    return datagen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize randomly transform data below. You can change the parameters above to generate more extreme random changes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plt_image(im):\n",
    "    plt.imshow(im)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    \n",
    "# Print a few random samples\n",
    "datagen1 = create_datagen()\n",
    "nplot = 9\n",
    "for i in range(0, 9):\n",
    "    plt.subplot(3,3,i+1)\n",
    "    plt_image(datagen1.random_transform(Xtr[I[i]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we run over all possible options:  The options are:\n",
    "        \n",
    "* `basic`:  Basic CNN, no batch normalization or dropout   \n",
    "* `bn`:  Basic CNN + batch normalization\n",
    "* `dropout`:  Basic CNN + batch normalization + dropout\n",
    "* `dataaug`:  Basic CNN + batch normalization + dropout + data augmentation\n",
    "\n",
    "This will take tens of minutes per model even on a GPU. \n",
    "\n",
    "We run each in a seperate code cells: otherwise Colab might time out for \"inactivity\" on a single block. First we set some common parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "nepochs = 100\n",
    "batch_size = 32\n",
    "lr = 1e-3\n",
    "decay = 1e-4\n",
    "\n",
    "# Create the optimizer\n",
    "opt = optimizers.RMSprop(lr=lr, decay=decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following line you can ignore. It was needed to properly use the current version of Tensorflow on my Macbook \n",
    "# due to issues with OpenMP. Leaving here in case it's useful for others.\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic model\n",
    "\n",
    "K.clear_session()\n",
    "model = create_mod()  \n",
    "\n",
    "# Compile\n",
    "hist = model.compile(loss='sparse_categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "print(model.summary()) \n",
    "\n",
    "# Fit the model\n",
    "hist = model.fit(Xtr, ytr, batch_size=batch_size, epochs=nepochs, validation_data=(Xts, yts), shuffle=True)\n",
    "\n",
    "# Save history\n",
    "mod_name = 'basic'\n",
    "hist_fn = ('hist_%s.p' % mod_name)\n",
    "with open(hist_fn, 'wb') as fp:\n",
    "    hist_dict = hist.history\n",
    "    pickle.dump(hist_dict, fp) \n",
    "print('History saved as %s' % hist_fn)                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bn model (batch normalization add)\n",
    "\n",
    "# Create the basic CNN model\n",
    "K.clear_session()\n",
    "model = create_mod(use_bn=True)\n",
    "\n",
    "# Compile\n",
    "hist = model.compile(loss='sparse_categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "print(model.summary()) \n",
    "\n",
    "# Fit the model\n",
    "hist = model.fit(Xtr, ytr, batch_size=batch_size, epochs=nepochs, validation_data=(Xts, yts), shuffle=True)\n",
    "\n",
    "# Save history\n",
    "mod_name = 'bn'\n",
    "hist_fn = ('hist_%s.p' % mod_name)\n",
    "with open(hist_fn, 'wb') as fp:\n",
    "    hist_dict = hist.history\n",
    "    pickle.dump(hist_dict, fp) \n",
    "print('History saved as %s' % hist_fn)                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropout model (batch normalization and dropout added)\n",
    "\n",
    "# Create the basic CNN model\n",
    "K.clear_session()\n",
    "model = create_mod(use_bn=True, use_dropout=True)   \n",
    "\n",
    "# Compile\n",
    "hist = model.compile(loss='sparse_categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "print(model.summary()) \n",
    "\n",
    "# Fit the model\n",
    "hist = model.fit(Xtr, ytr, batch_size=batch_size, epochs=nepochs, validation_data=(Xts, yts), shuffle=True)\n",
    "\n",
    "# Save history\n",
    "mod_name = 'dropout'\n",
    "hist_fn = ('hist_%s.p' % mod_name)\n",
    "with open(hist_fn, 'wb') as fp:\n",
    "    hist_dict = hist.history\n",
    "    pickle.dump(hist_dict, fp) \n",
    "print('History saved as %s' % hist_fn)                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## NOTE: For some reason running model.fit with a data generator is not working in Google Colab right now.\n",
    "## I'm having trouble diagoning why, so have commented out this part of the lab for now.\n",
    "\n",
    "'''\n",
    "# dataaug model (data augmentation added)\n",
    "\n",
    "# Create the basic CNN model\n",
    "K.clear_session()\n",
    "model = create_mod(use_bn=True, use_dropout=True) \n",
    "\n",
    "# Compile\n",
    "hist = model.compile(loss='sparse_categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "print(model.summary()) \n",
    "\n",
    "# Create a data augmentation object\n",
    "datagen = create_datagen()\n",
    "\n",
    "# Fit the model\n",
    "hist = model.fit(datagen.flow(Xtr, ytr, batch_size=batch_size), epochs=nepochs, validation_data=(Xts,yts)) \n",
    "\n",
    "# Save history\n",
    "mod_name = 'dataaug'\n",
    "hist_fn = ('hist_%s.p' % mod_name)\n",
    "with open(hist_fn, 'wb') as fp:\n",
    "    hist_dict = hist.history\n",
    "    pickle.dump(hist_dict, fp) \n",
    "print('History saved as %s' % hist_fn) \n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now plot the results.  You should approximately get\n",
    "\n",
    "* `baseline` ~71%\n",
    "* `bn` ~75%\n",
    "* `bn+dropout` ~79%\n",
    "\n",
    "So batch normalization and dropout help.  Note that `baseline` and `bn` get training accuracies of ~100% suggesting overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_name_plot = ['basic', 'bn', 'dropout']\n",
    "plt.figure(figsize=(10,5))\n",
    "for iplt in range(2):\n",
    "    \n",
    "    plt.subplot(1,2,iplt+1)\n",
    "    for i, mod_name in enumerate(mod_name_plot):\n",
    "\n",
    "        # Load history\n",
    "        hist_fn = ('hist_%s.p' % mod_name)\n",
    "        with open(hist_fn, 'rb') as fp:        \n",
    "            hist_dict = pickle.load(fp) \n",
    "\n",
    "        acc = hist_dict['val_accuracy']\n",
    "        plt.plot(acc, '-', linewidth=3)\n",
    "    \n",
    "    n = len(acc)\n",
    "    nepochs = len(acc)\n",
    "    plt.grid()\n",
    "    plt.xlim([0, nepochs])\n",
    "    plt.legend(['baseline', 'bn', 'bn+dropout', 'bn+dropout+aug'])\n",
    "    plt.xlabel('Epoch')\n",
    "    if iplt == 0:\n",
    "        plt.ylabel('Train accuracy')\n",
    "    else:\n",
    "        plt.ylabel('Test accuracy')\n",
    "        \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print final accuracies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, mod_name in enumerate(mod_name_plot):\n",
    "\n",
    "    # Load history\n",
    "    hist_fn = ('hist_%s.p' % mod_name)\n",
    "    with open(hist_fn, 'rb') as fp:        \n",
    "        hist_dict = pickle.load(fp) \n",
    "        \n",
    "    # Print average of last 5 acc\n",
    "    acc = hist_dict['val_accuracy']\n",
    "    print('%15s :  %5.3f' % (mod_name, np.mean(acc[-5:])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
