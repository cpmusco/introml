{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2D Convolution and Convoltional Layers\n",
    "\n",
    "In this demo, you will learn to:\n",
    "* Compute 2D convolutions on images using `scipy`.\n",
    "* Visualize the outputs of convolutions\n",
    "* Create a convolutional layer in Keras\n",
    "* Mathematically describe the weights with multiple input and output channels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and Displaying Images\n",
    "\n",
    "We first import several packages.  Two important packages we will use are: \n",
    "* `scipy.signal`: sub-package which has useful routines for 2D convolutions; and\n",
    "* `skimage.data`: sub-package for getting some basic images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.signal\n",
    "import skimage.data\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We next create a function to display images that we will use throughout the demo.  The function does both gray-scale and color images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the image\n",
    "def disp_image(im):\n",
    "    if (len(im.shape) == 2):\n",
    "        # Gray scale image\n",
    "        plt.imshow(im, cmap='gray')    \n",
    "    else:\n",
    "        # Color image.  \n",
    "        im1 = (im-np.min(im))/(np.max(im)-np.min(im))*255\n",
    "        im1 = im1.astype(np.uint8)\n",
    "        plt.imshow(im1)    \n",
    "        \n",
    "    # Remove axis ticks\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `skimage.data` package has several commonly used images in image processing.  We begin by loading a widely-used \"cameraman\" image.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im = skimage.data.camera()\n",
    "disp_image(im)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We print the image shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The image shape is: \"+str(im.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also load images from files using the `skimage.io.imread` function.  This function can handle most common image formats including, for example, JPG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = 'still_life.jpg'\n",
    "im_color = skimage.io.imread(fname)\n",
    "disp_image(im_color)\n",
    "print(\"The image shape is: \"+str(im_color.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing 2D Convolutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To illustrate 2D convolution, we first apply an averaging filter, which essentially replaces each pixel by the average of a neighborhood of `KxK` pixels to the top left of the current pixel. Filtering an image with this filter has a smoothing effect.  In image processing, convolving with an averaging filter can be useful as a crude noise removal filter. But, for convolutional networks, an averaging kernel finds regions of high intensity.\n",
    "\n",
    "Not that we use the `correlate2d()` function.  The `scipy.signal` package also has a `convolve2d()` function.  But, in signal processing, the term *convolution* includes a sign reversal which is not included in deep networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kx = 9\n",
    "ky = 9\n",
    "sig = 3\n",
    "G_unif = np.ones((kx,ky))/(kx*ky)\n",
    "im_unif_full = scipy.signal.correlate2d(im, G_unif, mode='full')\n",
    "im_unif_same = scipy.signal.correlate2d(im, G_unif, mode='same')\n",
    "im_unif_valid = scipy.signal.correlate2d(im, G_unif, mode='valid')\n",
    "\n",
    "# Plot the original image and the three outputs\n",
    "plt.figure(figsize=(13,13))\n",
    "plt.subplot(1,4,1)\n",
    "disp_image(im)\n",
    "plt.title('Original')\n",
    "plt.subplot(1,4,2)\n",
    "disp_image(im_unif_full)\n",
    "plt.title('Uniform kernel, Full')\n",
    "plt.subplot(1,4,3)\n",
    "disp_image(im_unif_same)\n",
    "plt.title('Uniform kernel, Same')\n",
    "plt.subplot(1,4,4)\n",
    "disp_image(im_unif_valid)\n",
    "plt.title('Uniform kernel, Valid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us look at the image sizes. Let `K` represent filter width, `Ni` image width, `No` output width. Then:\n",
    "\n",
    "* `full`: `No = Ni+K-1`, \n",
    "* `same`:  `No=Ni`\n",
    "* `valid`:  `No=Ni-K+1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Input shape = \" + str(im.shape))\n",
    "print(\"Output shape (Full) = \" + str(im_unif_full.shape))\n",
    "print(\"Output shape (Same) = \" + str(im_unif_same.shape))\n",
    "print(\"Output shape (valid) = \" + str(im_unif_valid.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us take a closer look at the output near the boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the original image and the three outputs at the top left corner\n",
    "plt.figure(figsize=(13,13))\n",
    "plt.subplot(1,4,1)\n",
    "plt.imshow(im[0:20,0:20], vmin=0, vmax=255, cmap='gray')\n",
    "plt.title('Original')\n",
    "plt.subplot(1,4,2)\n",
    "plt.imshow(im_unif_full[0:20,0:20], vmin=0, vmax=255, cmap='gray')\n",
    "plt.title('Uniform kernel, Full')\n",
    "plt.subplot(1,4,3)\n",
    "plt.imshow(im_unif_same[0:20,0:20], vmin=0, vmax=255, cmap='gray')\n",
    "plt.title('Uniform kernel, Same')\n",
    "plt.subplot(1,4,4)\n",
    "plt.imshow(im_unif_valid[0:20,0:20], vmin=0, vmax=255, cmap='gray')\n",
    "plt.title('Uniform kernel, Valid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us look at the effect of increasing the average window size. As we can see, the larger is `K`, the more bluured the image becomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kx = 9\n",
    "ky = 9\n",
    "\n",
    "G_unif = np.ones((kx,ky))/(kx*ky)\n",
    "im_unif_9 = scipy.signal.correlate2d(im, G_unif, mode='same')\n",
    "kx = 15\n",
    "ky = 15\n",
    "G_unif = np.ones((kx,ky))/(kx*ky)\n",
    "im_unif_15 = scipy.signal.correlate2d(im, G_unif, mode='same')\n",
    "\n",
    "# Plot the original image and the three outputs\n",
    "plt.figure(figsize=(13,13))\n",
    "plt.subplot(1,3,1)\n",
    "disp_image(im)\n",
    "plt.title('Original')\n",
    "plt.subplot(1,3,2)\n",
    "disp_image(im_unif_9)\n",
    "plt.title('Uniform kernel, 9x9')\n",
    "plt.subplot(1,3,3)\n",
    "disp_image(im_unif_15)\n",
    "plt.title('Uniform kernel, 15x15')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared to the averaging filter, the Gaussian kernel also performs an averaging, but weights the pixels at the center of the kernel more. You can control the bluring strength with the standard deviation sigma of the Gaussian kernel. Window size should generally be larger than 2 sigma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gauss_kernel(nx,ny,sig):\n",
    "    \"\"\"\n",
    "    Creates a Gaussian kernel of size (nx,ny) with std deviation sig\n",
    "    \"\"\"\n",
    "    dxsq = (np.arange(nx)-(nx-1)/2)**2\n",
    "    dysq = (np.arange(ny)-(ny-1)/2)**2\n",
    "    dsq = dxsq[:,None] + dysq[None,:]\n",
    "    G = np.exp(-0.5*dsq/(sig**2))\n",
    "    G = G / np.sum(G)\n",
    "    return G\n",
    "\n",
    "# Create a Gaussian kernel\n",
    "kx = 9\n",
    "ky = 9\n",
    "sig = 3\n",
    "G_unif = np.ones((kx,ky))/(kx*ky)\n",
    "\n",
    "# Create a Gaussian kernel\n",
    "G_gauss = gauss_kernel(kx,ky,sig)\n",
    "plt.imshow(G_gauss,interpolation='None',cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us compare effect of averaging and Gaussian filtering, with different sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlate with 9 x 9 Gaussian\n",
    "kx = 9\n",
    "ky = 9\n",
    "sig = 3\n",
    "G_gauss = gauss_kernel(kx,ky,sig)\n",
    "im_gauss_9= scipy.signal.correlate2d(im, G_gauss, mode='same')\n",
    "\n",
    "# Correlate with 15 x 15 Gaussian\n",
    "kx = 15\n",
    "ky = 15\n",
    "sig = 6\n",
    "G_gauss = gauss_kernel(kx,ky,sig)\n",
    "im_gauss_15= scipy.signal.correlate2d(im, G_gauss, mode='same')\n",
    "\n",
    "\n",
    "plt.figure(figsize=(13,13))\n",
    "plt.subplot(1,4,1)\n",
    "disp_image(im_unif_9)\n",
    "plt.title('Uniform kernel, 9x9')\n",
    "plt.subplot(1,4,2)\n",
    "disp_image(im_gauss_9)\n",
    "plt.title('Gaussian kernel, 9x9')\n",
    "plt.subplot(1,4,3)\n",
    "disp_image(im_unif_15)\n",
    "plt.title('Uniform kernel, 15x15')\n",
    "plt.subplot(1,4,4)\n",
    "disp_image(im_gauss_15)\n",
    "plt.title('Gaussian kernel, 15x15')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addtion to noise removal, a common use of convolution is edge detection by using a filter that approximates the operation of gradient in horizontal and vertical directions. One such filter set is called the [Sobel filters](https://en.wikipedia.org/wiki/Sobel_operator), `Gx` and `Gy`.  The filter `Gx` filter will have a large response when there is a large change in the x-direction (vertical edge).  Similarly, `Gy` will have a large response for a large change in the image in the y-direction (horizontal edge).   In any part of the image that is flat, the output is zero. Without filter reversal, the response is positive if the change is from black to white."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Gx = np.array([[1,0,-1],[2,0,-2],[1,0,-1]])  # Gradient operator in the x-direction\n",
    "Gy = np.array([[1,2,1],[0,0,0],[-1,-2,-1]])  # Gradient operator in the y-direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the convolutions\n",
    "imx = scipy.signal.correlate2d(im, Gx, mode='valid')\n",
    "imy = scipy.signal.correlate2d(im, Gy, mode='valid')\n",
    "\n",
    "# Plot the original image and the two outputs\n",
    "plt.figure(figsize=(13,13))\n",
    "plt.subplot(1,3,1)\n",
    "disp_image(im)\n",
    "plt.title('Original')\n",
    "plt.subplot(1,3,2)\n",
    "disp_image(imx)\n",
    "plt.title('Gx * image')\n",
    "plt.subplot(1,3,3)\n",
    "disp_image(imy)\n",
    "plt.title('Gy * image')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us see the result with the flipped filter. As can be seen, with filter reversal, the response is positive if the change is from white to black. This is in fact the result of correlating the filter mask with the local neighborhood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing a Convolutional Layer in Keras\n",
    "\n",
    "We now show how to implement a convolutional layer in Keras.  Typically, in Keras, images are represented as 4-th order tensors with dimensions `(batch_size,height,width,nchannels)`, where `batch_size` is the number of images in a batch, `(height,width)` is the images size per channel and `nchannels` is the number of image channels.  For BW images, there is only one channel. We first reshape the image to this format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrow, ncol = im.shape \n",
    "nimage = 1         # number of images in batch\n",
    "nchan_in = 1       # number of input channels.  1 since it is BW\n",
    "input_shape = (nrow,ncol,nchan_in)  # input shape of 1 image\n",
    "batch_shape = (nimage,nrow,ncol,nchan_in)  # shape of image batch\n",
    "x = im.reshape(batch_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we import the appropriate Keras packages.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Input, Conv2D\n",
    "import tensorflow.keras.backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now create a simple network with one convolutional layer and two output channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "model = Sequential()\n",
    "kernel_size = Gx.shape\n",
    "nchan_out = 2\n",
    "model.add(Conv2D(input_shape=input_shape,filters=nchan_out,\n",
    "                 kernel_size=kernel_size,name='conv2d'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weights are represented as a `(3,3,1,2)` which correspond to `(krow,kcol,nchan_in,nchan_out)`.  Thus, for each input and output channel pair, there is one kernel of size `(krow,kcol)`. The bias has size `(nchan_out,)` corresponding to one bias value per output channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = model.get_layer('conv2d')\n",
    "W, b = layer.get_weights()\n",
    "print(\"W shape = \" + str(W.shape))\n",
    "print(\"b shape = \" + str(b.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set the two filters using the Sobel filters `Gx` and `Gy`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W[:,:,0,0] = Gx\n",
    "W[:,:,0,1] = Gy\n",
    "b = np.zeros(nchan_out)\n",
    "layer.set_weights((W,b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now run the image through the convolutional layer and display the outputs.  We can see that it produces the same outputs as we did earlier using convolve2D function, when we flipped the filters. This is because the convolution inside the Tensorflow actually implements \"correlation\". Also, the output image size with the default setting is equivalent to using the `valid` option.\n",
    "\n",
    "Note that you can choose to use other boundary treatment by setting the ``padding`` option properly:\n",
    "\n",
    "*model.add(Conv2D(input_shape=input_shape,filters=nchan_out,\n",
    "                 kernel_size=kernel_size,name='conv2d', padding='same'))*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = im.reshape(batch_shape)\n",
    "y = model.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the original image and the two outputs\n",
    "chan0=y[0,:,:,0];\n",
    "chan1=y[0,:,:,1];\n",
    "\n",
    "plt.figure(figsize=(13,13))\n",
    "plt.subplot(1,3,1)\n",
    "disp_image(im)\n",
    "plt.title('Input')\n",
    "plt.subplot(1,3,2)\n",
    "disp_image(chan0)\n",
    "plt.title('chan 0 out')\n",
    "plt.subplot(1,3,3)\n",
    "disp_image(chan1)\n",
    "plt.title('chan 1 out')\n",
    "\n",
    "print(chan0.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Multiple Input and Output Channels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now create a keras model with a single convolutional layer to process the color image.  We first get the dimensions of the color image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disp_image(im_color)\n",
    "nrow,ncol,nchan_in = im_color.shape\n",
    "nimage = 1\n",
    "input_shape = (nrow,ncol,nchan_in)\n",
    "batch_shape = (nimage,nrow,ncol,nchan_in)\n",
    "print(\"Image shape is \"+str(im_color.shape))\n",
    "\n",
    "# Reshape image to a batch\n",
    "x = im_color.reshape(batch_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now create a network with a single convolutional layer with `nchan_out=4` output channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimensions\n",
    "nchan_out = 4\n",
    "kernel_size = (9,9)\n",
    "\n",
    "# Create network\n",
    "K.clear_session()\n",
    "model = Sequential()\n",
    "model.add(Conv2D(input_shape=input_shape,filters=nchan_out,\n",
    "                 kernel_size=kernel_size,name='conv2d'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check: The number of parameters for each output channel is nchan_in  kernel_size +1 (bias). The total number for nchan_out is then nchan_out (nchan_in  kernel_size+1)=4 (3 81+1)=976!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, the weight matrix is of size `(krow,kcol,nchan_in,nchan_out)` so that there is one kernel of size `(krow,kcol)` per input-output channel pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = model.get_layer('conv2d')\n",
    "W, b = layer.get_weights()\n",
    "print(W.shape)\n",
    "print(b.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To illustrate the 2D convolutions, we create kernels of the form:\n",
    "\n",
    "    W[a,b,i,j] = G[a,b]*color_wt[j,i]\n",
    "    \n",
    "where `color_wt[i,j]` represents the weighting of color channel `i` for output channel `j` and `G[:,:]` is a uniform kernel over space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Color weights\n",
    "color_wt = np.array([\n",
    "    [1,    -0.5, -0.5],   # Sensitive to red\n",
    "    [-0.5,    1, -0.5],   # Sensitive to green\n",
    "    [-0.5, -0.5,    1],   # Sensitive to blue\n",
    "    [ 0.5,   -1,  0.5],   # Sensitive to red-blue mix\n",
    "])\n",
    "\n",
    "# Gaussian kernel over space\n",
    "krow, kcol = kernel_size\n",
    "G = gauss_kernel(krow,kcol,sig=2)\n",
    "\n",
    "# Multiply by weigthing color\n",
    "W = G[:,:,None,None]*color_wt.T[None,None,:,:]\n",
    "b = np.zeros(b.shape)\n",
    "layer.set_weights((W,b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot the filters.  The filter for each output channel is itself a small RGB image and indicates which local feature it is sensitive to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(nchan_out):\n",
    "    plt.subplot(1,nchan_out,i+1)\n",
    "    disp_image(W[:,:,:,i])\n",
    "    title_str = 'chan %d' % i\n",
    "    plt.title(title_str,fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now run the image through the network.  We see that the output feature maps clearly indicate the different colors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = model.predict(x)\n",
    "plt.figure(figsize=(20,20))\n",
    "for i in range(nchan_out):\n",
    "    plt.subplot(2,nchan_out,i+1)\n",
    "    disp_image(y[0,:,:,i])\n",
    "    title_str = 'chan %d' % i\n",
    "    plt.title(title_str,fontsize=16)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
